# 자율주행차에 관한 설명



### 개요

이것은 Siraj Raval의 유튜브 [영상](https://youtu.be/yt015gM-ync)에 관한 코드입니다. 시뮬레이터는 [여기](https://github.com/udacity/self-driving-car-sim)에서 찾을 수 있습니다. 

이 프로젝트의 목적은 딥러닝 신경망을 통해 인간의 운전 행동을 똑같이 따라하는 것입니다. 이러한 목적을 달성하기 위해, 간단한 자동차 시뮬레이터를 사용합니다. 훈련 단계에서는 키보드를 사용하여 시뮬레이터 안의 차량을 조종합니다. 차량을 조종하는 동안 시뮬레이터는 트레이닝 이미지와 그때의 핸들 각도를 측정합니다. 그리고 신경망을 훈련시키는 데 그 기록 데이터를 사용합니다. 훈련된 모델은 두 개의 트랙, 즉 훈련 트랙과 검증 트랙에서 테스트됩니다. 아래 애니메이션들은 훈련 트랙과 검증 트랙에서 최종 모델의 성과를 보여줍니다.

훈련 | 검증
------------|---------------
![training_img](./images/track_one.gif) | ![validation_img](./images/track_two.gif)

### 의존성

이 프로젝트는 **Python 3.5**, 그리고 다음과 같은 Python 라이브러리의 설치가 필요합니다:

- [Keras](https://keras.io/)
- [NumPy](http://www.numpy.org/)
- [SciPy](https://www.scipy.org/)
- [TensorFlow](http://tensorflow.org)
- [Pandas](http://pandas.pydata.org/)
- [OpenCV](http://opencv.org/)
- [Matplotlib](http://matplotlib.org/) (Optional)
- [Jupyter](http://jupyter.org/) (Optional)

터미널 프롬프트에서 다음 커맨드를 입력하여 [OpenCV](http://opencv.org/)를 설치합니다. 이는 이미지 처리에 사용됩니다:
```
conda install -c https://conda.anaconda.org/menpo opencv3
```

### 모델 실행 방법

다음 커맨드를 사용하여, 이미 훈련된 모델을 사용할 수 있습니다.

```
python drive.py model.json
```

## 구현

### 데이터 수집

훈련 중에는 시뮬레이터가 10hz의 빈도로 데이터를 수집합니다. 또한 매 시간마다 좌, 우, 중앙 카메라의 이미지를 기록합니다. 다음 이미지들은 훈련 중 수집한 예시입니다.

좌측| 중앙 | 우측
----|--------|-------
![left](./images/left.png) | ![center](./images/center.png) | ![right](./images/right.png)

수집된 데이터는 딥러닝 모델에 입력되기 전에 전처리 과정을 거칩니다. 전처리 과정은 아래에 설명되어 있습니다.

### 데이터 집합 통계
데이터 집합은 24,108개의 이미지 (카메라 당 8,036개의 이미지)로 구성되어 있습니다. 훈련 트랙은 다수의 약한 커브길과 직선 도로로 구성되어 있습니다. 따라서 대부분의 핸들 각도 기록은 0입니다. 그러므로 훈련된 모델이 검증 트랙과 같이 접해보지 못한 트랙에서도 범용적으로 잘 동작려면, 이미지와 그에 대응하는 핸들 각도를 전처리하는 것이 필수적입니다.

다음으로 데이터처리 파이프라인에 대해 설명드리도록 하겠습니다.

### 데이터 처리 파이프라인
아래 그림은 데이터처리 파이프라인입니다.

<p align="center">
 <img src="./images/pipeline.png" width="550">
</p>

파이프라인의 첫 단계에서는 이미지를 무작위로 늘려 변형시킵니다. 90%의 확률로 적용하며 10%에 해당되는 원본 이미지와 핸들 각도는 그대로 보존하여 훈련 트랙에서 차량을 조종할 수 있도록 합니다. 다음 그림은 샘플 이미지를 늘려 변형한 결과를 보여줍니다.

<p align="center">
 <img src="./images/sheared.png">
</p>

시뮬레이터에 의해 수집된 이미지에는 모델을 빌드하는 데 직접적으로 도움이 되지 않는 상세한 정보들이 많습니다. 이런 상세 정보들이 차지하는 추가 영역에도 많은 처리가 필요하기 때문에, 이미지의 상단 35%, 하단 10%를 제거합니다. 이 과정은 crop 단계에서 이루어집니다. 다음 그림은 crop 단계의 결과물입니다.

<p align="center">
 <img src="./images/cropped.png">
</p>

데이터 처리 파이프라인의 다음 단계는 무작위로 뒤집는 (flip) 작업입니다. 이 단계에서 0.5의 확률로 이미지를 뒤집습니다. 이 작업은 훈련 트랙에서 좌회전 구간이 우회전 구간보다 더 많다는 점에서 착안하였습니다. 그러므로 모델을 일반화시키기 위해서는 이미지와 그에 따른 핸들 각도도 뒤집어야 합니다. 다음 그림은 뒤집기를 적용된 결과입니다.

<p align="center">
 <img src="./images/flipped.png">
</p>

파이프라인의 마지막 단계에서는 훈련 시간을 줄이기 위해 이미지의 크기를 `64 * 64`로 리사이즈하는 것입니다. 리사이즈된 샘플 이미지는 아래 그림에서 볼 수 있습니다. 리사이즈된 이미지들이 신경망으로 입력되게 됩니다. 다음은 리사이즈가 적용된 결과입니다.

<p align="center">
 <img src="./images/resized.png">
</p>

다음으로 신경망의 아키텍쳐를 알아봅시다.

### 신경망 아키텍쳐 

이 CNN 아키텍쳐는 NVIDIA의 \"자율 주행 차량을 위한 end to end 학습\" 논문에서 영감을 얻었습니다. 이 모델이 NVIDIA의 것과 다른 점은, 훈련 시간을 줄이기 위해 Max-pooling층을 각 합성곱 층의 바로 다음에 넣은 것입니다. 더 자세한 정보는 다음 그림을 참고하시기 바랍니다.

<p align="center">
 <img src="./images/conv_architecture.png" height="600">
</p>

### 훈련
이미지의 Crop과 리사이즈를 거쳐도, 훈련 데이터셋은 너무 커서 메모리에 다 들어가지 못합니다. 따라서 모델을 훈련시키는 데 Keras의 `fit_generator` API를 사용하였습니다.

두 개의 생성기를 만들었습니다:
* `train_gen = helper.generate_next_batch()`
* `validation_gen = helper.generate_next_batch()` 

배치 사이즈는 `train_gen`과 `validation_gen` 모두 64입니다. 트레이닝 epoch 별로 20,032개의 이미지를 사용하였습니다. 학습용 이미지는 위에서 설명한 데이터 처리 파이프라인에 따라 생성되었습니다. 또한, 6,400개의 생성된 이미지를 검증에 사용하였습니다. `1e-4`의 학습률로 `Adam` 옵티마지어를 사용하였고, 트레이닝 epoch는 5, 8, 10, 25, 50을 사용해 보았는데, `8`이 훈련과 검증 트랙에서 모두 잘 작동하였습니다.

## 결과
프로젝트의 초기 단계에서는 제가 직접 만든 데이터셋을 사용하였습니다. 그건 노트북 키보드를 사용하여 주행하는 동안 기록한 작은 데이터셋이었습니다. 그러나 이것으로 빌드한 모델은 시뮬레이터에서 자율주행하는 데 충분하지 않았습니다. 하지만 이후 Udacity에 올라온 데이터셋을 사용하였고, 여기에 추가로 데이터을 늘려 추가하여 개발된 모델은 다음 비디오에서 볼 수 있듯 잘 작동했습니다.

#### 훈련 트랙
[![training_track](https://img.youtube.com/vi/nSKA_SbiXYI/0.jpg)](https://www.youtube.com/watch?v=nSKA_SbiXYI)

#### 검증 트랙
[![validation_track](https://img.youtube.com/vi/ufoyhOf5RFw/0.jpg)](https://www.youtube.com/watch?v=ufoyhOf5RFw)

## 결론 및 향후 방향
이 프로젝트에서는 자율주행차라는 맥락에서 회귀 문제를 다루었습니다. 첫 단계에서는 적합한 신경망 아키텍쳐를 찾는 것과 데이터셋으로 모델을 훈련시키는 데 집중하였으며, 평균제곱오차(**MSE**) 관점에서는 잘 작동하였지만 실제 시뮬레이터에서 모델을 테스트할 때는 기대만큼 잘 동작하지 못했습니다. 따라서 이 프로젝트에서 평균제곱오차는 실제 성능을 측정하는 데 좋은 지표가 아니라고 결론지을 수 있습니다.

다음 단계에서 Udacity에서 업로드한 새로운 데이터셋을 사용하였고, 또한 평균제곱오차에 전적으로 의존하지 않았으며, 상대적으로 작은 훈련 epoch를 사용하였습니다(`8` epochs). 새로운 데이터셋에 데이터셋을 늘려 넣어 학습한 모델은 놀라울 정도로 잘 작동하였고 최종 모델은 두 트랙에서 최고 성능을 보였습니다. 

확장성 및 향후 방향성에 관해 다음 내용을 강조하고 싶습니다.

* 실제 도로 조건에서 모델을 훈련시킵니다. 이를 위해서는 새로운 시뮬레이터를 찾아야 할 수도 있습니다.
* 다른 데이터 증강 기술을 적용하여 실험합니다.
* 우리가 자동차를 운전할 때 핸들을 돌리거나 브레이크를 밟는 행위 등은 그 순간의 운전 결정만을 기초로 하는 것이 아닙니다. 사실 운전 시 의사결정은 몇 초 전의 교통 상황이나 도로 상황 등이 무엇이었느냐에 좌우됩니다. 따라서 **LSTM**이나 **GRU** 등의 순환 신경망(**RNN**)이 이 문제를 어떻게 해결하는지 지켜보는 것도 흥미로울 것입니다.
* 마지막으로, 심층 강화학습 또한 추가적인 프로젝트로서 흥미로운 주제가 될 수 있습니다.


## 크레딧

이 코드의 크레딧은 [upul](https://github.com/upul/Behavioral-Cloning)에 있습니다. 
